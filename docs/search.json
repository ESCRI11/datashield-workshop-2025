[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DataSHIELD Workshop: Setting up the infrastructure",
    "section": "",
    "text": "Introduction\nThis site hosts materials for a hands‑on workshop on deploying and managing Opal and Armadillo. We will cover local deploy of the infrastructure, manage of profiles and profile creation. As extra material we will have a look at strategies of monitoring of the infrastructure. Expect practical Linux/SSH usage from a terminal and a copy‑pasteable flow you can follow on your laptop. The materials of this workshop, especially the live deploy part are applicable to cloud deployments as well.\n\n\nWorkshop Details\nThis workshop will be delivered at the DataSHIELD 2025 Conference hosted by the SIB Swiss Institute of Bioinformatics at the University of Lausanne (UniL), Switzerland, on Tuesday, September 23rd from 13:30 to 16:30 at room POL-338.\n\n\nGetting started\n\nReview your setup: Environment Setup\nSkim the essentials: Get up to speed\nOpal vs Armadillo: Difference\n\n\n\nWorkshop scope\n\nDeploy Opal and Armadillo with Docker Compose (no Kubernetes)\nNginx reverse proxy: go live with Let’s Encrypt\nBasic Linux/SSH workflows suitable for on‑prem and cloud\nOptional: lightweight monitoring with Telegraf/Prometheus + Grafana (time permitting)\n\n\n\nWho this is for\n\nBeginners to sysadmin tasks; we explain each step\nAttendees on macOS (Apple Silicon ok), Windows (Docker Desktop/WSL2), or Linux\nFollow‑along is encouraged but not mandatory\n\n\n\nPrerequisites (summary)\nSee details in Environment Setup. In short:\n\nDocker Desktop (Mac/Windows) or Docker Engine + Compose v2 (Linux)\nTerminal + SSH access; ability to open ports 80/443 locally\nOptional domain name if you want to test live certificates\n\n\n\nSchedule\nTo be announced.\n\n\nCredits\nMaterials developed by Dick Postma (Molgenis) and Xavier Escribà Montagut (BigOmics Analytics SA).\n\n\nContact\nDick Postma\nf.d.postma at umcg.nl\nXavier Escribà Montagut\nxavier.escriba.montagut at gmail.com",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Introduction</span>"
    ]
  },
  {
    "objectID": "env_setup.html",
    "href": "env_setup.html",
    "title": "Environment Setup",
    "section": "",
    "text": "Prerequisites",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Environment Setup</span>"
    ]
  },
  {
    "objectID": "env_setup.html#prerequisites",
    "href": "env_setup.html#prerequisites",
    "title": "Environment Setup",
    "section": "",
    "text": "Operating System\n\nWindows Users: Ensure you have WSL2 installed. This allows you to run a Linux environment directly on Windows, which is necessary for Docker.\nMac and Linux Users: You are good to go with your native terminal.\nReference System: This workshop material has been tested on Ubuntu 24.04.3 LTS\n\n\n\nSoftware Requirements\n\nDocker Engine (&gt;= 28.3.3) with Docker Compose\n\nWindows/macOS: Docker Desktop bundles Docker Compose.\nLinux: Install Docker Engine and Docker Compose separately.\nRequired version: Docker Compose v2.39.2 or later\n\nGit (&gt;= 2.43.0)\n\nUsed for cloning workshop repositories and version control\nDownload from git-scm.com\nAlternative: GitHub Desktop for a graphical interface\nVerify installation: git --version\n\nTerminal or shell (PowerShell, Windows Terminal, macOS Terminal, Linux shell)\nText Editor/IDE: VS Code, Sublime, Vim, or similar.\nR (&gt;= 4.3.3) and RStudio Desktop (optional but recommended for client-side checks)\n\n\n\nHardware requirements\n\nAt least 4 CPU cores and 8 GB RAM recommended (4 GB minimum)\n10+ GB free disk space for images, containers, and volumes\n\n\n\nNetwork and permissions\n\nAdministrative rights to install software\nUnrestricted internet access to docker.io, ghcr.io and GitHub\nVPN/proxy configured for Docker if required by your organization\nVirtualization enabled in BIOS/UEFI (Intel VT-x/AMD-V or Apple virtualization)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Environment Setup</span>"
    ]
  },
  {
    "objectID": "env_setup.html#install-docker",
    "href": "env_setup.html#install-docker",
    "title": "Environment Setup",
    "section": "Install Docker",
    "text": "Install Docker\n\nWindows (WSL2)\n\nInstall and enable WSL2: Microsoft guide\nInstall Docker Desktop: Docker Desktop for Windows\nIn Docker Desktop settings, enable “Use the WSL 2 based engine” and integration with your WSL distro\n\n\n\nmacOS\n\nInstall Docker Desktop: Docker Desktop for Mac\n\n\n\nLinux\n\nInstall Docker Engine following the official docs: Install Docker Engine\nInstall Docker Compose (standalone): Install Docker Compose\n\n\nLinux post-install (non-root usage)\n# Add your user to the docker group and activate it\nsudo usermod -aG docker $USER\n# Restart your terminal to activate the changes",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Environment Setup</span>"
    ]
  },
  {
    "objectID": "env_setup.html#verify-your-setup",
    "href": "env_setup.html#verify-your-setup",
    "title": "Environment Setup",
    "section": "Verify your setup",
    "text": "Verify your setup\nRun the following commands. All should succeed without errors.\ndocker --version\ndocker-compose --version\ndocker run --rm hello-world\n\nExpected output (or newer versions):\nDocker version 28.3.3, build 980b856 Docker Compose version v2.39.2-desktop.1\n\nExpected output (or newer versions):\nDocker version 28.3.3, build 980b856\nDocker Compose version v2.39.2-desktop.1",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Environment Setup</span>"
    ]
  },
  {
    "objectID": "env_setup.html#additional-notes",
    "href": "env_setup.html#additional-notes",
    "title": "Environment Setup",
    "section": "Additional Notes",
    "text": "Additional Notes\n\nEnsure your Docker engine is allocated with sufficient memory (at least 4GB; 8GB recommended) to avoid performance issues.\nClose applications using typical service ports you plan to map (e.g., 8080, 8443) to avoid conflicts.\nOn corporate networks, confirm Docker can pull images from public registries.\nFamiliarize yourself with basic Docker commands and docker-compose operations, as we will be interacting with the console during the workshop.\nImportant: We use docker-compose (with hyphen) throughout this workshop, not docker compose (space-separated).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Environment Setup</span>"
    ]
  },
  {
    "objectID": "env_setup.html#learn-docker-quick-references",
    "href": "env_setup.html#learn-docker-quick-references",
    "title": "Environment Setup",
    "section": "Learn Docker: quick references",
    "text": "Learn Docker: quick references\n\nDocker Getting Started: docs.docker.com/get-started\nDocker CLI reference: docs.docker.com/engine/reference/commandline/docker\nDocker Compose overview: docs.docker.com/compose\nVolumes and persistence: docs.docker.com/storage/volumes\nPlay with Docker (hands-on labs): labs.play-with-docker.com\n\nBy following these steps, your environment will be ready for the workshop. If you encounter any issues, feel free to reach out to the workshop organizers for assistance.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Environment Setup</span>"
    ]
  },
  {
    "objectID": "up_2_speed.html",
    "href": "up_2_speed.html",
    "title": "Get up to speed",
    "section": "",
    "text": "This workshop is a hands-on primer on deploying secure, federated DataSHIELD backends with Docker Compose. We will focus on Opal and Armadillo deployments with Rock servers; covering local development, production deployment with TLS, and profile management.\n\nWhat you’ll learn\n\nDeploy Opal with Rock servers using Docker Compose (local → production)\nDeploy Armadillo as an alternative DataSHIELD backend\nConfigure Nginx reverse proxy: local self‑signed → Let’s Encrypt\nManage multiple DataSHIELD profiles for different research contexts\nBuild custom Rock server images with specific package versions\nPractical Linux/SSH and terminal workflows (great for cloud)\nProduction deployment strategies: IT-managed vs self-managed\n\n\n\nCourse structure\n\nArmadillo local deployment: Quick start with Armadillo + RServer\nOpal local deployment: Core focus - Opal + Rock + MongoDB locally\nGoing live: Production deployment with Nginx, TLS certificates, and DNS\nManaging profiles: Multiple DataSHIELD environments\nCustom Rock images: Building reproducible, version-controlled server images\n\n\n\nAudience and setup\n\nSkill level: beginner-friendly. We’ll explain each step as we go.\nFollow‑along encouraged on your laptop (Mac/Windows/Linux). If you prefer to watch, you’ll still get all commands and files.\n\n\n\nPrerequisites checklist\n\nLaptop: macOS (Apple Silicon is fine), Windows 10/11, or Linux\nContainers:\n\nmacOS/Windows: install Docker Desktop and ensure both docker and docker-compose work\n\nDocker Desktop: docs\n\nLinux: install Docker Engine and Docker Compose (we’ll use docker-compose)\n\nDocker Engine install: docs\nDocker Compose install: docs\n\n\nTerminal + SSH: basic comfort with shell, ssh user@host\nPorts:\n\nLocal: allow Docker to bind 80/443 (stop other services using these ports)\nCloud (optional): if testing live TLS, ensure 22/80/443 are open\n\nDomain (optional, for live demo): have a test DNS name ready if you want to issue real certificates with Let’s Encrypt; otherwise we’ll stick to self‑signed locally\n\n\n\nTarget platforms we’ll mention\n\nOn‑prem demo first; cloud notes included\nCloud suggestion: Ubuntu 24.04 LTS on x86_64 for quick starts\n\n\n\nWhat we’ll deploy (at a glance)\n\nOpal stack\n\nLocal development: Opal/Armadillo + Rock + MongoDB with HTTP access\nProduction deployment: Nginx reverse proxy with Let’s Encrypt TLS\nMultiple profiles: Default, survival analysis, genomics environments\nCustom images: Version-controlled Rock servers with specific packages\n\n\n\nArmadillo stack\n\nArmadillo server: Alternative DataSHIELD backend\nRServer integration: DataSHIELD package execution environment\n\n\n\nDeployment progression\n\nStage 1 (local): HTTP access for development and testing\nStage 2 (production): HTTPS with proper certificates and DNS\nStage 3 (advanced): Multiple profiles and custom package management\n\n\n\n\n\n\n\nTip\n\n\n\nNo prior sysadmin experience required. We’ll keep commands copy‑pasteable and explain the “why” briefly as we go.\n\n\n\n\n\nWhy start with local deployment?\n\nLocal keeps friction low: no DNS, firewalls, or cloud costs. You can validate the stack in minutes on any laptop.\nSafe sandbox: use HTTP and avoid exposing services to the internet while you explore features.\nDeveloper‑friendly: iterate on DataSHIELD code and packages against a predictable, reproducible environment.\nPortable: the same Compose foundation scales up later (add DNS, TLS, and hardening without changing core services).\nTeaching aid: a contained lab to learn Opal and Rock.\n\n\n\nPre‑reading (short and optional)\n\nDataSHIELD overview paper: International Journal of Epidemiology\nOpal (DataSHIELD server) overview: OBiBa Opal\nArmadillo documentation: MOLGENIS Armadillo\nDataSHIELD packages documentation: cran.datashield.org\nDocker basics for this workshop:\n\nDocker Desktop (Mac/Windows): docs\nCompose v2 usage/migration: docs\n\nLet’s Encrypt on Ubuntu + Nginx (for production deployment):\n\nCertbot + Nginx (Ubuntu): DigitalOcean guide\n\n\n\n\nWhat we will not cover\n\nKubernetes deployments (we focus on Docker Compose)\nAdvanced identity management (OIDC/Keycloak)\nFirewall hardening beyond TLS basics\nAdvanced monitoring and logging (though we will see mention Telegraf/Prometheus + Grafana if time permits)\n\n\n\nOutcomes\nBy the end, you’ll have:\n\nA working local Opal/Armadillo deployment for development and testing\nUnderstanding of production deployment options (IT-managed vs self-managed)\nKnowledge of DataSHIELD profile management for different research contexts\nSkills to build and maintain custom Rock server images\nA clear path from development to production deployment",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Get up to speed</span>"
    ]
  },
  {
    "objectID": "difference.html",
    "href": "difference.html",
    "title": "Armadillo vs Opal",
    "section": "",
    "text": "What they are",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Armadillo vs Opal</span>"
    ]
  },
  {
    "objectID": "difference.html#what-they-are",
    "href": "difference.html#what-they-are",
    "title": "Armadillo vs Opal",
    "section": "",
    "text": "Opal\n\nMature platform by OBiBa\nFull data management: storage, harmonisation, metadata, cataloguing\nWidely used in DataSHIELD documentation and consortia\n\nArmadillo\n\nLightweight server by MOLGENIS team (Support & Development)\nBuilt specifically for DataSHIELD federated analysis\nEasier deployment, fewer dependencies",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Armadillo vs Opal</span>"
    ]
  },
  {
    "objectID": "difference.html#architecture-storage",
    "href": "difference.html#architecture-storage",
    "title": "Armadillo vs Opal",
    "section": "Architecture & Storage",
    "text": "Architecture & Storage\n\nOpal\n\nUses relational DBs (MySQL, PostgreSQL, MariaDB) or MongoDB\nStores data and metadata in databases\nSupports complex workflows\n\nArmadillo\n\nStores data on the filesystem (e.g. Parquet files)\nUploads via UI or R package\nSimpler, less infrastructure heavy",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Armadillo vs Opal</span>"
    ]
  },
  {
    "objectID": "difference.html#features",
    "href": "difference.html#features",
    "title": "Armadillo vs Opal",
    "section": "Features",
    "text": "Features\n\nOpal\n\nRich functionality: harmonisation, metadata, cataloguing\nMultiple R servers, horizontal scaling\nAdvanced admin and access control\n\nArmadillo\n\nFocused on core federated analysis features\nProfiles: named/versioned DS package collections\nPermissions and function whitelisting\nLightweight and user-friendly",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Armadillo vs Opal</span>"
    ]
  },
  {
    "objectID": "difference.html#pros-cons",
    "href": "difference.html#pros-cons",
    "title": "Armadillo vs Opal",
    "section": "Pros & Cons",
    "text": "Pros & Cons\n\nOpal\n\n✅ Mature and feature-rich\n✅ Good for large, complex infrastructures\n❌ Higher complexity and overhead\n\nArmadillo\n\n✅ Lightweight, quick to deploy\n✅ Easier for cohorts with modest resources\n❌ Fewer features (e.g. less advanced harmonisation)\n❌ Newer, still catching up in some areas",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Armadillo vs Opal</span>"
    ]
  },
  {
    "objectID": "difference.html#when-to-use",
    "href": "difference.html#when-to-use",
    "title": "Armadillo vs Opal",
    "section": "When to use",
    "text": "When to use\n\nChoose Opal if:\n\nYou need full data management, metadata and harmonisation\nYou operate in a large consortium with complex workflows\n\nChoose Armadillo if:\n\nYou want lightweight, easy deployment\nYou mainly need secure federated analysis\nYour team has limited IT resources and want Molgenis Support",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Armadillo vs Opal</span>"
    ]
  },
  {
    "objectID": "1-local-deploy-armadillo.html",
    "href": "1-local-deploy-armadillo.html",
    "title": "1. Deploying Armadillo for local use",
    "section": "",
    "text": "Welcome",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>1. Deploying Armadillo for local use</span>"
    ]
  },
  {
    "objectID": "1-local-deploy-armadillo.html#welcome",
    "href": "1-local-deploy-armadillo.html#welcome",
    "title": "1. Deploying Armadillo for local use",
    "section": "",
    "text": "Run Armadillo, RServer, locally with Docker Compose\n\n\n\nServices and Roles\nWhat we Deploy:\n\nArmadillo → http://localhost:8000\n\nMain DataSHIELD server and admin UI. (With admin/admin as user)\n\nRServer → http://localhost:6311\n\nDataSHIELD packages;\n\n\n\n\n\nGoal\nDeploy Armadillo locally on your laptop using Docker Compose.\nReferences :\n\nArmadillo Quick Setup\nArmadillo running under systemd\n\n\n\n\nFiles layout\nlocal-deploy-armadillo\n├── config \n    ├── application.yml ( Main armadillo configuration ) \n├── data                ( Local storage ) \n├── docker-compose.yml \n└── logs                ( Armadillo logfiles ) \n\n\n\n1) Armadillo configuration\nDefine password and profiles in this file. If you are running armadillo under systemd you can enable docker-management-enabled to true.\narmadillo:\n  docker-management-enabled: false\n  docker-run-in-container: true\n  profiles:\n    - name: default\n      image: datashield/rock-base:latest\n      port: 8085 \n      host: rserver \n      package-whitelist:\n        - dsBase\n      function-blacklist: [ ]\n      options:\n        datashield:\n          seed: 342325352\n\n# required settings:\nspring:\n  security:\n    user:\n      # please change this admin password!\n      password: admin!!!\n\n  # optional settings (review spring handbook to find more):\n  servlet:\n    multipart:\n      ## change this if your files are bigger\n      max-file-size: 1000MB\n      max-request-size: 1000MB\n\nstorage:\n  ## to change location of the data storage\n  root-dir: /data\n\n# Match with Dockerfile volume /logs\naudit.log.path: '/logs/audit.log'\nstdout.log.path: '/logs/armadillo.log'\n\nlogging:\n  level:\n    root: INFO\n    ## change to DEBUG to have more details, typically when developing\n    org.molgenis: DEBUG\n    ## Don't log upload data\n    org.apache.coyote.http11.Http11InputBuffer: INFO\n\n\n\n2) docker-compose.yml\nDefines the armadillo docker config.\nservices:\n armadillo:\n   image: molgenis/molgenis-armadillo:latest\n   platform: linux/amd64\n   environment:\n     LOGGING_CONFIG: 'classpath:logback-file.xml'\n     SPRING_SECURITY_USER_PASSWORD: 'admin'\n     SPRING_RSERVER_URL: 'http://rserver:8085'\n     DEBUG: \"TRUE\"\n   ports:\n     - \"8000:8080\"\n   volumes:\n     - ./logs:/app/logs\n     - ./data:/data\n     - /var/run/docker.sock:/var/run/docker.sock\n     - ./config:/config\n   networks:\n     - armadillo-workshop\n\n rserver:\n   image: datashield/rock-base:latest\n   platform: linux/amd64\n   environment:\n     DEBUG: \"TRUE\"\n   networks:\n     - armadillo-workshop\n\nnetworks:\n  armadillo-workshop:\n    driver: bridge\n\n\n\n3) Bring up the stack\nFrom the directory containing ‘docker-compose’\n\ndocker compose up -d\n#firs start may take a minute while images are pulled.\n\n#check health\n\ndocker compose logs -f armadillo\n\n\n\n4) Check Armadillo UI\n\nOpen http://localhost:8000 in your browser.\nLogin with user admin and the password in the configuration.\n\n\n\n5) Rstudio Check R connectivity\nlibrary(dsBaseClient)\nlibrary(DSI)\nlibrary(DSMolgenisArmadillo)\n\n\n\nurl &lt;- \"http://localhost:8000\"\nbuilder &lt;- DSI::newDSLoginBuilder()\n\nbuilder$append(\n  server = \"armadillo\",\n  url = url,\n  #token = token,\n  user = \"admin\",\n  password = \"admin\",\n  driver = \"ArmadilloDriver\",\n  profile = \"default\")\n\nlogindata &lt;- builder$build()\nconns &lt;- DSI::datashield.login(logins = logindata)\n\n\nds.ls()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>1. Deploying Armadillo for local use</span>"
    ]
  },
  {
    "objectID": "1-local-deploy-armadillo.html#troubleshooting",
    "href": "1-local-deploy-armadillo.html#troubleshooting",
    "title": "1. Deploying Armadillo for local use",
    "section": "Troubleshooting",
    "text": "Troubleshooting\n\nReset the stack\n\ndocker compose down -v\ndocker compose up --build",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>1. Deploying Armadillo for local use</span>"
    ]
  },
  {
    "objectID": "1-local-deploy-armadillo.html#kubernetes-production",
    "href": "1-local-deploy-armadillo.html#kubernetes-production",
    "title": "1. Deploying Armadillo for local use",
    "section": "Kubernetes / Production",
    "text": "Kubernetes / Production\n\nFull Docker stack, and systemd setup available at: https://molgenis.github.io/molgenis-service-armadillo\nHelm charts available:\nhttps://github.com/molgenis/molgenis-service-armadillo/tree/master/helm-chart\nDataSHIELD kubernetes profiles maintained in Helm (private repo for some charts)\nIf you need help or want to contribute contact:\n\nEmail: support@molgenis.org\nSlack: join the #Armadillo-support channel (DataSHIELD Slack)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>1. Deploying Armadillo for local use</span>"
    ]
  },
  {
    "objectID": "1-local-deploy-armadillo.html#live-playground",
    "href": "1-local-deploy-armadillo.html#live-playground",
    "title": "1. Deploying Armadillo for local use",
    "section": "Live Playground",
    "text": "Live Playground\n\nMolgenis is providing an Armadillo playground. With an central analysis server playground (Jupyterhub). Contact support@molgenis.org with your e-mail adres for (institute) access.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>1. Deploying Armadillo for local use</span>"
    ]
  },
  {
    "objectID": "1-local-deploy-armadillo.html#notes",
    "href": "1-local-deploy-armadillo.html#notes",
    "title": "1. Deploying Armadillo for local use",
    "section": "Notes",
    "text": "Notes\n\nThis quickstart is intended for local development and testing\nFor production deployments prefer systemd or Kubernetes/Helm configurations",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>1. Deploying Armadillo for local use</span>"
    ]
  },
  {
    "objectID": "2-local-deploy-opal.html",
    "href": "2-local-deploy-opal.html",
    "title": "2. Deploying Opal for local use",
    "section": "",
    "text": "Architecture at a glance\n\n\n\n\n\ngraph LR\n  B[\"Browser&lt;br/&gt;http://localhost:8080\"] --&gt;|8080| O[\"Opal (8080)\"]\n  subgraph Profiles[\"docker network: opalnet\"]\n    O --&gt;|R/DataSHIELD| R[\"Rock (8085)\"]\n    O --&gt;|Auxiliary| MONGO[\"MongoDB (27017)\"]\n  end\n\n  %% Define a lighter background for the subgraph\n  classDef light fill:#f9f9f9,stroke:#aaa,stroke-width:1px;\n  class Profiles light;\n\n\n\n\n\n\n\n\nServices and roles\n\nOpal (OBiBa):\n\nMain DataSHIELD server and admin UI. Exposes HTTP on 8080.\nReads configuration via environment variables (e.g., OPAL_ADMINISTRATOR_PASSWORD, MONGO_*, ROCK_HOSTS) docs docs.\n\nRock (R server):\n\nExecutes R/DataSHIELD calls initiated by Opal, reachable as rock:8085 on the Docker network.\nShips with DataSHIELD packages; ideal for quick connectivity checks.\n\nMongoDB (auxiliary store):\n\nUsed by Opal for auxiliary features; we pin to mongo:6.0 per Opal documentationreference.\n\n\n\n\nGoal\nDeploy Opal locally on your laptop using Docker Compose. This lets you try DataSHIELD quickly without DNS or cloud setup and is perfect for package development and smoke tests. See the prerequisites in @env_setup.qmd and the scope in @up_2_speed.qmd.\n\n\nWhat we will deploy\n\nOpal server (obiba/opal:latest) with admin password set\nRock R server (datashield/rock-base:latest) for DataSHIELD execution\nMongoDB for auxiliary storage\n\nReferences:\n\nOpal Docker image + env vars: Installation — Docker Image and Configuration\n\n\n\nFiles layout\nUse this folder layout:\nopal-local/\n├── .env\n├── docker-compose.yml\n├── data/\n│   ├── opal/\n│   └── mongo/\n└── logs/\n\n\n1) Create a .env file\nKeep secrets out of the compose file.\nOPAL_ADMINISTRATOR_PASSWORD=ChangeMe123!\n\nOPAL_ADMINISTRATOR_PASSWORD is required on first start of obiba/opal.\n\n\n\n\n\n\n\nNote\n\n\n\nProduction note: For production deployments, the most correct way to handle sensitive information is through Docker secrets, Kubernetes secrets, or your platform’s native secrets management system. We use a .env file here for workshop simplicity, which also helps reduce the risk of password leaks when sharing docker-compose files to colleagues or collaborators.\n\n\n\n\n\n\n\n\nHow to use Docker secrets\n\n\n\n\n\nWe will explain briefly how to use Docker secrets to store the OPAL_ADMINISTRATOR_PASSWORD information in a secure way.\nIn order to create a Docker secret, you can run the following command:\necho -n \"ChangeMe123!\" | docker secret create opal_admin_password -\nThis will create a secret named opal_admin_password with the value ChangeMe123!.\nTo use the secret on the docker-compose.yml file, we have to modify it as follows:\nservices:\n  opal:\n    [...]\n    environment:\n      - MONGO_HOST=mongo\n      - MONGO_PORT=27017\n      - ROCK_HOSTS=rock:8085\n    secrets:\n      - opal_admin_password\n    [...]\n    entrypoint: &gt;\n      sh -c \"export OPAL_ADMINISTRATOR_PASSWORD=$$(cat /run/secrets/opal_admin_password) &&\n             exec /usr/local/bin/opal\"\nNote that secrets only work when we use Docker in swarm mode. We will have to run:\ndocker swarm init   # if not already done\ndocker stack deploy -c docker-compose.yml mystack\n\n\n\n\n\n2) docker-compose.yml\nDefines Opal, Rock, and MongoDB. Key env vars match Opal docs (MONGO_*, ROCK_HOSTS) opaldoc installation.\nservices:\n  opal:\n    image: obiba/opal:latest\n    depends_on:\n      - rock\n      - mongo\n    ports:\n      - \"8080:8080\"\n      - \"8443:8443\"\n    environment:\n      - OPAL_ADMINISTRATOR_PASSWORD=${OPAL_ADMINISTRATOR_PASSWORD}\n      - MONGO_HOST=mongo\n      - MONGO_PORT=27017\n      - ROCK_HOSTS=rock:8085\n    volumes:\n      - ./data/opal:/srv\n      - ./logs:/var/log/opal\n  mongo:\n    image: mongo:6.0\n    volumes:\n      - ./data/mongo:/data/db\n\n  rock:\n    image: datashield/rock-base:latest\n    environment:\n    - ROCK_ID=new-stack-rock\n\nnetworks:\n  default:\n    name: opalnet\nHighlights: - Opal sees Rock at rock:8085 and MongoDB via service names on the internal network. - Opal is directly accessible on port 8080 for simplicity. - All services run on the opalnet Docker network. - Data is persisted in local folders (./data/opal, ./data/mongo) instead of Docker volumes for easier management and portability.\n\n\n3) Bring it up\nFrom the directory containing docker-compose.yml and .env:\n# Create data directories\nmkdir -p data/opal data/mongo logs\n\n# Start services\ndocker-compose up -d\n# First start may take a minute while images are pulled.\n\n# Check health\ndocker-compose ps\ndocker-compose logs\nFor clean restarts (recommended for development):\n# Clean restart - ensures proper service registration\ndocker-compose down\ndocker-compose up -d\n\nOpen http://localhost:8080 in your browser.\nLogin with user administrator and the password you set in .env.\n\nIf you see the Opal UI and can navigate the Admin area, your stack is running.\n\n\n4) Minimal DataSHIELD test (R)\nVerify connectivity via DSOpal/DSI.\nlibrary(DSI)\nlibrary(DSOpal)\nlibrary(httr)\nset_config(config(ssl_verifyhost = 0L, ssl_verifypeer = 0L))\nlibrary(dsBaseClient)\n\n\nb &lt;- DSI::newDSLoginBuilder()\nb$append(\n    server   = \"local\",\n    url      = \"http://localhost:8080\",\n    user     = \"administrator\",\n    password = \"ChangeMe123!\",\n    profile = \"default\"\n)\n\nlogins &lt;- b$build()\nconns &lt;- DSI::datashield.login(logins)\nds.ls()\nNotes:\n\nThe Opal + Rock images are DataSHIELD‑ready by design. If a package appears missing, ensure Rock pulled the latest image or install required packages in Rock.\nFor real deployments, use HTTPS with proper certificates and omit the SSL verification overrides.\n\n\n\nTroubleshooting\n\nOpal not reachable: check if port 8080 is available locally.\nLogin fails: ensure OPAL_ADMINISTRATOR_PASSWORD was set at first start; to reset, run docker-compose down, remove the data/opal folder, and start again.\nMongo version: Opal expects MongoDB ≤ 6.0; using mongo:6.0 matches docs.\nRock connection issues: check that Rock container is running and healthy.\nProfile registration issues: use docker-compose down && docker-compose up -d for clean restarts that ensure proper service registration.\n\n\n\nNext steps\nWhen ready to go live, add Nginx reverse proxy with TLS, DNS configuration, and hardening as we’ll cover in the later section. For now, you have a working local Opal + DataSHIELD lab.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>2. Deploying Opal for local use</span>"
    ]
  },
  {
    "objectID": "3-going-live.html",
    "href": "3-going-live.html",
    "title": "3. Going live: cloud deployment",
    "section": "",
    "text": "Goal\nTake your local DataShield deployment public! We’ll deploy Opal + Rock + MongoDB to a live server with automatic SSL certificates, proper security, and a production-ready reverse proxy. By the end, you’ll have a fully functional DataShield server accessible from anywhere on the internet.\n\n\nTwo paths to going public\nWhen you’re ready to make your DataShield deployment publicly accessible, you have two main options:\n\nPath 1: Research Center IT (Recommended for most users)\n\nWhat you do: Deploy DataShield locally on a server provided by your research center (using the setup from section 2)\nWhat IT does: Configure firewall rules, domain names, SSL certificates, and network access\nBest for: Most research scenarios where your institution has dedicated IT support\nPros: IT handles security, compliance, and infrastructure concerns\nCons: Requires coordination with IT department, may have institutional restrictions\n\n\n\nPath 2: Full Cloud Deployment (What we’ll demonstrate)\n\nWhat you do: Everything - from server setup to SSL certificates to security configuration\nBest for: Cloud deployments (AWS, Azure, GCP), personal projects, or when you need full control\nPros: Complete control, faster iteration, great for learning\nCons: You’re responsible for security, updates, and maintenance\n\nIn this workshop, we’ll demonstrate Path 2 using AWS because it shows the complete process and gives you full understanding of all components. However, in practice, Path 1 is often the most appropriate choice for research environments.\n\n\n\nWhat we’ll build today\nWe’re going to transform your local setup into a production-ready deployment with:\n\nAutomatic SSL certificates from Let’s Encrypt\nReverse proxy serve your application via https\nProper domain access - your colleagues can access it! via https://your-domain.com (not a weird IP address)\nAll the security bells and whistles that make IT departments happy (hopefully)\n\n\n\nArchitecture (public)\n\n\n\n\n\ngraph LR\n  U[\"User&lt;br/&gt;https://your-domain.com\"] --&gt;|DNS resolves| N[\"Nginx (80/443)\"]\n  N --&gt;|proxy https-&gt;http| O[\"Opal (8080)\"]\n  subgraph Profiles[\"docker network: opalnet\"]\n    O --&gt;|R/DataSHIELD| R[\"Rock (8085)\"]\n    O --&gt;|Data storage| MONGO[\"MongoDB (27017)\"]\n  end\n  ACME[\"Certbot webroot&lt;br/&gt;/.well-known/acme-challenge/\"] -.-&gt;|HTTP-01| N\n\n  %% Define a lighter background for the subgraph\n  classDef light fill:#f9f9f9,stroke:#aaa,stroke-width:1px;\n  class Profiles light;\n\n\n\n\n\n\n\n\nPrerequisites\n\nRegistered domain name, e.g. opal.example.org\nDNS A/AAAA record pointing to your server’s public IP\nPorts 80 and 443 open to the internet (cloud SG/firewall/ufw)\nLinux host recommended (Ubuntu 22.04/24.04), Docker + Compose v2\n\n\n\nArchitecture transformation\nWe’re taking your simple local setup and adding production-grade components:\n\nNginx (NEW!): Acts as a security guard and traffic director\n\nHandles SSL certificates\nAdds security headers to protect against attacks\nRate limits to prevent abuse (optional)\n\nOpal: Your existing DataShield administration server (now behind the proxy)\nRock: Your existing R computation server (unchanged)\nMongoDB: Your existing database backend (unchanged)\nCertbot (NEW!): Automatically gets and renews SSL certificates from Let’s Encrypt\n\n\n\nFiles you’ll be working with\nDon’t worry - it’s not as complex as it looks! All of these files are provided in our live deployment scripts:\ndatashield-live/\n├── .env                    # 🔧 YOU EDIT: Just your domain name and password\n├── docker-compose.yml      # 📋 PROVIDED: Orchestrates all services\n├── nginx-template.conf     # 📋 PROVIDED: Nginx configuration with SSL\n├── nginx-http-only.conf    # 📋 PROVIDED: Temporary config for getting certificates\n├── get-certs.sh            # 🚀 PROVIDED: One-click SSL certificate setup\n└── renew-certs.sh          # 🔄 PROVIDED: Automatic certificate renewal\nYou literally only need to edit ONE file (.env) - everything else is ready to go!\n\n\n1) Environment Configuration\nThis is the only file you need to edit! Create your .env file:\n# DNS Configuration - CHANGE THIS to your actual domain!\nDNS_DOMAIN=datashield.myresearch.org\n# Opal Configuration - CHANGE THIS to a strong password!\nOPAL_ADMINISTRATOR_PASSWORD=SuperSecurePassword123!\n\n\n\n\n\n\nImportant\n\n\n\nDomain setup is crucial! Before proceeding, make sure your domain’s DNS A record points to your server’s public IP address. You can check this with:\nnslookup datashield.myresearch.org\n# Should return your server's IP address\n\n\n\n\n\n\n\n\nTip\n\n\n\nPassword security tip: Use a password manager to generate a strong password. This will be the admin password for your DataShield server, so make it count!\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs mentioned before, in production, you should use Docker secrets, Kubernetes secrets, or your platform’s native secrets management system. We use a static file here for simplicity.\n\n\n\n\n2) Docker Compose Configuration\nThe docker-compose.yml includes nginx reverse proxy, automatic SSL certificates, and all DataShield services:\nservices:\n  nginx:\n    image: nginx:alpine\n    depends_on:\n      - opal\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - ./nginx-template.conf:/etc/nginx/nginx-template.conf:ro\n      - ./ssl:/etc/ssl:ro\n    environment:\n      - DNS_DOMAIN=${DNS_DOMAIN}\n    command: /bin/sh -c \"sed \\\"s/DNS_DOMAIN_PLACEHOLDER/$${DNS_DOMAIN}/g\\\" /etc/nginx/nginx-template.conf &gt; /etc/nginx/nginx.conf && nginx -g 'daemon off;'\"\n    restart: unless-stopped\n\n  certbot:\n    image: certbot/certbot:latest\n    volumes:\n      - ./ssl:/etc/letsencrypt\n    environment:\n      - DNS_DOMAIN=${DNS_DOMAIN}\n    command: certonly --webroot --webroot-path=/var/www/certbot --register-unsafely-without-email --agree-tos --no-eff-email --expand -d ${DNS_DOMAIN}\n\n  opal:\n    image: obiba/opal:latest\n    depends_on:\n      - rock\n      - mongo\n    environment:\n      - OPAL_ADMINISTRATOR_PASSWORD=${OPAL_ADMINISTRATOR_PASSWORD}\n      - MONGO_HOST=mongo\n      - MONGO_PORT=27017\n      - ROCK_HOSTS=rock:8085\n    volumes:\n      - ./data/opal:/srv\n      - ./logs:/var/log/opal\n\n  mongo:\n    image: mongo:6.0\n    volumes:\n      - ./data/mongo:/data/db\n\n  rock:\n    image: datashield/rock-base:latest\n    environment:\n      - ROCK_ID=new-stack-rock\n\nnetworks:\n  default:\n    name: opalnet\n\n\n3) AWS Setup\n\nEC2 Instance Setup\n\nLaunch an EC2 instance (recommended: t3.medium or larger)\nConfigure Security Group rules:\n\nPort 80 (HTTP) - open to 0.0.0.0/0\nPort 443 (HTTPS) - open to 0.0.0.0/0\nPort 22 (SSH) - open to your IP\n\n\n\n\nDomain Configuration\nPoint your domain DNS to the EC2 instance public IP:\nA record: your-domain.com -&gt; YOUR_EC2_PUBLIC_IP\n\n\n\n4) The Magic Deployment Process ✨\nReady for the exciting part? Let’s go live!\n\nStep 1: Configure your environment\n# Edit the .env file with your domain and password\nnano .env\n# (Or use your favorite editor: vim, code, etc.)\n\n\nStep 2: The three-step dance to production! 🕺\n# 🚀 Step 1: Start the core DataShield services\necho \"Starting DataShield services...\"\ndocker-compose up -d mongo rock opal\n\n# ⏳ Wait a moment for services to initialize\necho \"Services starting... (this takes about 30 seconds)\"\nsleep 30\n\n# 🔒 Step 2: Get your shiny SSL certificates!\necho \"Getting SSL certificates from Let's Encrypt...\"\n./get-certs.sh\n\n# 🔄 Step 3: Restart nginx with the new certificates\necho \"Configuring nginx with SSL certificates...\"\ndocker-compose stop nginx\ndocker-compose rm -f nginx\ndocker-compose up -d nginx\n\necho \"🎉 Deployment complete!\"\n\n\nStep 3: The moment of truth!\nOpen your browser and visit https://your-domain.com\nYou should see:\n\n🔒 A beautiful green lock icon (SSL is working!)\n🏠 The familiar Opal login page\n🎯 No browser security warnings\n\nLog in with:\n\nUsername: administrator\nPassword: The password you set in .env\n\nIf you see the Opal dashboard, congratulations! 🎉 You just deployed DataShield to production!\n\n\n\n5) Automatic renewal\nLet’s Encrypt certs expire every ~90 days. Renew periodically and reload Nginx.\n# Try a dry run first\ndocker compose run --rm certbot renew --dry-run -w /var/www/certbot\n\n# Example cron (host): renew daily at 03:00 and reload nginx\n# crontab -e\n0 3 * * * cd /path/to/opal-live && docker compose run --rm certbot renew -w /var/www/certbot && docker compose exec nginx nginx -s reload &gt;&gt; certbot-renew.log 2&gt;&1\nIf port 80 cannot be opened, consider DNS-01 challenges (requires DNS provider API integration) instead of HTTP-01.\n\n\n6) Testing your live deployment 🧪\n\nWeb Access\n\n🏠 Main DataShield Interface: https://your-domain.com\n❤️ Health Check: https://your-domain.com/health (should return “healthy”)\n\n\n\nYour new credentials\n\nUsername: administrator\nPassword: Whatever you set in .env as OPAL_ADMINISTRATOR_PASSWORD\n\n\n\nThe R test that proves it’s working\nNow for the real test - connecting from R with proper SSL security (no more scary certificate warnings!):\n# Install packages if you haven't already\n# install.packages(c(\"DSI\", \"DSOpal\", \"dsBaseClient\"))\n\nlibrary(DSI)\nlibrary(DSOpal)\nlibrary(dsBaseClient)\n\n# 🎉 Notice: No more SSL verification overrides needed!\n# Your server now has a proper certificate!\n\n# Set up your connection\nb &lt;- DSI::newDSLoginBuilder()\nb$append(\n  server   = \"production\",\n  url      = \"https://your-domain.com\",  # 🔒 HTTPS with real certificate!\n  user     = \"administrator\", \n  password = \"youpassword!\" # From .env\n)\n\n# Connect and test\nlogins &lt;- b$build()\nconns &lt;- DSI::datashield.login(logins)\n\nds.ls()\n🎯 Success indicators:\n\nNo SSL certificate warnings in R\nConnection establishes without errors\nYour browser shows a green lock icon\n\n\n\nShare with colleagues!\nYour DataShield server is now accessible to collaborators anywhere in the world:\n\nJust share the URL: https://your-domain.com\nThey can use the same R connection code (with their own credentials)\nNo VPN or special network setup required!\n\n\n\n\nTroubleshooting & hardening\n\nDNS propagation can take time. Check: dig +short opal.example.org resolves to your IP.\nEnsure 80/443 reach your host (cloud SG, ufw, on-prem firewall). Test: curl -I http://opal.example.org/.well-known/acme-challenge/test (should hit Nginx).\nSet strong TLS only (we used TLS1.2/1.3 and HIGH ciphers) and enable HSTS.\nConsider limiting Nginx request sizes, enabling access logs, and fail2ban/WAF if exposed to the internet.\n\nReferences:\n\nOpal Docker image and configuration: Installation — Docker Image, Configuration\nNginx/Certbot flow: DigitalOcean — Secure Nginx with Let’s Encrypt (Ubuntu)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>3. Going live: cloud deployment</span>"
    ]
  },
  {
    "objectID": "4-add-profiles.html",
    "href": "4-add-profiles.html",
    "title": "4. Managing profiles",
    "section": "",
    "text": "Goal\nLearn how to add new DataSHIELD profiles to your local Opal deployment from section 1. This builds directly on the simple local setup, showing you how to extend it with multiple profiles without losing data or adding production complexity.\n\n\nWhat are profiles?\nProfiles in Opal/DataSHIELD are named configurations that can include:\n\nSpecific sets of DataSHIELD packages\nDifferent R package versions\nCustom package repositories\nEnvironment-specific configurations\n\nThink of profiles as isolated R environments within your Opal deployment, similar to Python virtual environments, but for DataSHIELD research contexts.\n\n\nPrerequisites\n\nWorking local Opal deployment from section 1\nBasic understanding of Docker Compose\nYour local deployment should be running and accessible at http://localhost:8080\n\n\n\nCommon use cases for profiles\n\nResearch-specific environments: Different studies requiring different package versions\nDevelopment vs. stable: Testing new packages before using them in research\nPackage version management: Maintaining stable versions while testing updates\nLearning: Experimenting with different DataSHIELD packages safely\n\n\n\nArchitecture with profiles (local)\n\n\n\n\n\ngraph TB\n  B[\"Browser&lt;br/&gt;http://localhost:8080\"] --&gt; O[\"Opal Server (8080)\"]\n  O --&gt; R1[\"Rock Profile: default&lt;br/&gt;dsBase\"]\n  O --&gt; R2[\"Rock Profile: genomics&lt;br/&gt;dsOmics, dsExposure\"] \n  O --&gt; R3[\"Rock Profile: survival&lt;br/&gt;dsSurvival\"]\n  subgraph Profiles[\"docker network: opalnet\"]\n    R1 --&gt; P1[\"rock-default:8085\"]\n    R2 --&gt; P2[\"rock-genomics:8085\"] \n    R3 --&gt; P3[\"rock-survival:8085\"]\n  end\n\n  %% Define a lighter background for the subgraph\n  classDef light fill:#f9f9f9,stroke:#aaa,stroke-width:1px;\n  class Profiles light;\n\n\n\n\n\n\n\n\nStarting point: Your local deployment\nFrom section 1, you should have this structure:\nopal-local/\n├── .env\n├── docker-compose.yml\n├── data/\n│   ├── opal/\n│   └── mongo/\n└── logs/\nWe’ll extend this by adding new Rock services for each profile.\n\n\nStep 1: Update your .env file\nYour .env file from section 1 only needs the password. No changes required:\nOPAL_ADMINISTRATOR_PASSWORD=ChangeMe123!\n\n\nStep 2: Extend docker-compose.yml with profiles\nReplace your docker-compose.yml from section 1 with this extended version that adds a survival profile:\nservices:\n  opal:\n    image: obiba/opal:latest\n    depends_on:\n      - rock-default\n      - rock-survival\n      - mongo\n    ports:\n      - \"8080:8080\"\n      - \"8443:8443\"\n    environment:\n      - OPAL_ADMINISTRATOR_PASSWORD=${OPAL_ADMINISTRATOR_PASSWORD}\n      - MONGO_HOST=mongo\n      - MONGO_PORT=27017\n      # Multiple Rock hosts - comma separated\n      - ROCK_HOSTS=rock-default:8085,rock-survival:8085\n    volumes:\n      - ./data/opal:/srv\n      - ./logs:/var/log/opal\n\n  # Default profile (same as section 1, just renamed)\n  rock-default:\n    image: datashield/rock-base:latest\n    environment:\n      - ROCK_ID=default\n\n  # New survival analysis profile  \n  rock-survival:\n    image: datashield/rock-base:latest\n    environment:\n      - ROCK_ID=survival\n\n  # MongoDB (unchanged from section 1)\n  mongo:\n    image: mongo:6.0\n    volumes:\n      - ./data/mongo:/data/db\n\nnetworks:\n  default:\n    name: opalnet\n\nUnderstanding the changes\nLet’s break down what changed from the simple setup in section 1 and why:\n1. Modified Opal Service Dependencies\ndepends_on:\n  - rock-default\n  - rock-survival  # NEW\n  - mongo\nThis ensures Opal waits for all Rock profile containers to start before attempting to connect to them, preventing connection errors during startup.\n2. Updated ROCK_HOSTS Environment Variable\n- ROCK_HOSTS=rock-default:8085,rock-survival:8085\nThis comma-separated list tells Opal where to find all available Rock servers. Opal uses this to discover and manage multiple DataSHIELD computation environments simultaneously.\n3. Renamed Original Rock Service\n# From: rock (section 1) \n# To:   rock-default\nrock-default:\n  environment:\n    - ROCK_ID=default  # Explicit profile name\n\nConsistency: All profiles now follow the same naming pattern (rock-{profile-name})\nClarity: Makes it explicit that this is the “default” profile\nROCK_ID: Each Rock container needs a unique identifier for Opal to distinguish between profiles\n\n4. Added New Profile Services\nrock-survival:\n  environment:\n    - ROCK_ID=survival  # Unique profile identifier\nEach profile runs in its own container, providing isolated R environments. The unique ROCK_ID allows Opal to route DataSHIELD operations to the correct profile.\n5. What Stays the Same\n\nMongoDB service: Data storage is independent of computation profiles\nData persistence: Local folders preserve your existing data during the transition\nNetwork: Same opalnet network ensures all services can communicate\n\nThis transforms your architecture from a single computation environment to multiple isolated profiles:\nBefore: Opal ↔ Single Rock Container\n\nAfter:  Opal ↔ Multiple Rock Containers (profiles)\n        ├── rock-default (dsBase)\n        └── rock-survival (dsBase,dsSurvival)\n\n\n\nStep 3: Apply changes with maintenance window\n\n\n\n\n\n\nWarning\n\n\n\nService Interruption: Adding or removing profiles requires stopping the Opal service, which will temporarily interrupt access for all users. Plan a maintenance window and notify users in advance.\n\n\nThe key to adding profiles without losing your existing data is using local folders that persist between container recreations:\n# 1. Stop and remove all services (data in local folders is preserved)\ndocker-compose down\n\n# 2. Update your docker-compose.yml file (copy the extended version above)\n\n# 3. Start the new configuration\ndocker-compose up -d\n\n# 4. Verify all services are healthy\ndocker-compose ps\nBenefits of this approach: Using docker-compose down ensures clean container recreation, proper service registration, and profile discovery while preserving all data in local folders (./data/opal, ./data/mongo, ./logs).\n\n\nStep 4: Verify your profiles are running\nCheck that all Rock containers are running:\n# Check container status\ndocker-compose ps\n\n# Check logs for each profile\ndocker-compose logs rock-default\ndocker-compose logs rock-genomics  \ndocker-compose logs rock-survival\nYou should see all three Rock containers running and healthy.\n\n\nAdding a single profile (step-by-step)\nIf you want to add just one profile at a time, here’s the simplified process:\n\n1) Update docker-compose.yml\nAdd the new Rock service to the services: section:\n      rock-newprofile:\n    image: datashield/rock-base:latest\n    environment:\n      - ROCK_ID=newprofile\nAnd update the ROCK_HOSTS environment variable in the opal service:\n- ROCK_HOSTS=rock-default:8085,rock-genomics:8085,rock-survival:8085,rock-newprofile:8085\n\n\n2) Apply the changes (with maintenance window)\n# Stop and remove all services (clean restart)\ndocker-compose down\n\n# Start with new configuration\ndocker-compose up -d\n\n\n\nRemoving a profile\nTo remove a profile safely:\n\n1) Update docker-compose.yml\n\nDelete the entire rock-genomics: service block\nRemove rock-genomics:8085 from the ROCK_HOSTS environment variable\n\n\n\n2) Apply the changes (with maintenance window)\n# Stop and remove all services (clean restart)\ndocker-compose down\n\n# Start with updated configuration\ndocker-compose up -d\n\n\n\n\n\n\nNote\n\n\n\nMaintenance Planning: Profile changes require a brief service interruption. For production deployments, schedule these changes during low-usage periods and communicate the maintenance window to users.\n\n\n\n\n\nTesting your profiles from R\nVerify each profile works correctly:\nlibrary(DSI)\nlibrary(DSOpal)\nlibrary(httr)\n# Disable SSL verification for local testing\nset_config(config(ssl_verifyhost = 0L, ssl_verifypeer = 0L))\n\n# Test genomics profile\nbuilder &lt;- DSI::newDSLoginBuilder()\nbuilder$append(\n  server = \"genomics\",\n  url = \"http://localhost:8080\",\n  user = \"administrator\", \n  password = \"ChangeMe123!\",\n  driver = \"Opal\",\n  profile = \"genomics\"  # Specify the profile\n)\n\nlogins &lt;- builder$build()\nconns &lt;- DSI::datashield.login(logins)\n\n# Check available packages\nDSI::datashield.pkg_status(conns)\n\nDSI::datashield.logout(conns)\n\n\nTroubleshooting profiles\n\nProfile not appearing in Opal UI\n# Check Rock container is running\ndocker-compose ps\n\n# Check Rock container logs for errors\ndocker-compose logs rock-genomics\n\n# Test network connectivity from Opal to Rock\ndocker-compose exec opal curl -f http://rock-genomics:8085/\n\n\nConnection timeouts\n\nVerify the ROCK_HOSTS environment variable includes all profiles\nCheck that service names match between docker-compose.yml and ROCK_HOSTS\nEnsure all containers are on the same network (opalnet)\nTry a clean restart: docker-compose down && docker-compose up -d\n\n\n\nMemory issues\n# Monitor container resource usage\ndocker stats\n\n# If running low on memory, consider:\n# 1. Reducing the number of active profiles\n# 2. Stopping unused profiles temporarily\ndocker-compose stop rock-genomics\n\n\n\nNext steps\nWith profiles working locally, you can:\n\nInstall different packages in each profile using the Opal UI\nCreate project-specific profile assignments\nTest new DataSHIELD packages safely in development profiles\nScale up to production deployment (sections 2 and beyond) when ready\n\nThis local multi-profile setup gives you a powerful development environment for DataSHIELD research while keeping complexity manageable.\n\n\nReferences\n\nOpal DataSHIELD configuration: DataSHIELD Administration\nRock server documentation: Rock Server\nDocker Compose services: Docker Compose Services",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>4. Managing profiles</span>"
    ]
  },
  {
    "objectID": "5-creating-a-rock-server.html",
    "href": "5-creating-a-rock-server.html",
    "title": "5. Creating custom Rock server images",
    "section": "",
    "text": "Goal\nLearn how to build custom Rock server Docker images with specific DataSHIELD packages and versions. This allows you to create reproducible, version-controlled profiles that can be shared across deployments.\n\n\nWhy build custom Rock images?\nWhile the previous section showed how to add profiles using the standard obiba/rock:latest image, building custom images provides several advantages:\n\nVersion control: Pin specific package versions for reproducibility\nConsistency: Ensure all environments use identical package versions\n\n\n\nPrerequisites\n\nDocker installed and running\nBasic understanding of Dockerfiles\nUnderstanding of R package dependencies\n\n\n\nUnderstanding the base image\nDataSHIELD provides base images that you can extend:\n\ndatashield/rock-base:6.3-R4.3: Minimal Rock server with R 4.3\nobiba/rock:latest: Standard Rock just with resourcer\n\n\n\n\n\n\n\nNote\n\n\n\nDataSHIELD base images follow the pattern {dsBase-version}-R{R-version}. In the example above, 6.3-R4.3 means dsBase version 6.3 with R version 4.3.\n\n\nFor custom builds, start with rock-base for maximum control over package versions.\n\n\nExample: Survival analysis profile\nHere’s a complete example for creating a survival analysis Rock image:\n\n1) Create the Dockerfile\nDockerfile.survival:\n#\n# Rock R Server Dockerfile with DataSHIELD Survival profile\n#\n# Based on: https://github.com/datashield/docker-rock\n#\n\nFROM datashield/rock-base:6.3-R4.3\n\n# Define package versions for reproducibility\nENV DSURVIVAL_VERSION v2.3.0-dev\n\n# Rock library path\nENV ROCK_LIB /var/lib/rock/R/library\n\n# Install DataSHIELD packages with specific versions\n\n# dsSurvival (survival analysis functions)\nRUN Rscript -e \"remotes::install_github('datashield/dsSurvival', ref = '$DSURVIVAL_VERSION', dependencies = TRUE, upgrade = FALSE, lib = '$ROCK_LIB')\"\n\n# Fix ownership (Rock runs as non-root user)\nRUN chown -R rock $ROCK_LIB\n\n\n2) Build the image\n# Build the image with a descriptive tag\ndocker build -f Dockerfile.survival -t rock-survival:v2.3.0 .\n\n# Alternative: Build with multiple tags\ndocker build -f Dockerfile.survival \\\n  -t rock-survival:v2.3.0 \\\n  -t rock-survival:latest \\\n  .\n\n\n3) Test the image locally\n# Run the custom image\ndocker run -d --name test-survival-rock \\\n  -p 8085:8085 \\\n  rock-survival:v2.3.0\n\n# Check that packages are installed\ndocker exec test-survival-rock Rscript -e \"library(dsSurvival); packageVersion('dsSurvival')\"\n\n# Clean up\ndocker stop test-survival-rock\ndocker rm test-survival-rock\n\n\n\nImage management and distribution\n\nTagging strategy\nUse semantic versioning for your custom images:\n# Development versions\ndocker build -t rock-survival:v2.3.0-dev .\n\n# Release versions\ndocker build -t rock-survival:v2.3.0 .\ndocker build -t rock-survival:latest .\n\n# Environment-specific tags\ndocker build -t rock-survival:production .\ndocker build -t rock-survival:staging .\n\n\nPushing to a registry\nA Docker registry is a centralized repository for storing and distributing Docker images. It allows you to share images across different environments and with other developers. Docker Hub is the most popular free registry service provided by Docker, offering public repositories for open-source projects and private repositories for proprietary code.\n# Tag for your registry\ndocker tag rock-survival:v2.3.0 your-registry.com/rock-survival:v2.3.0\n\n# Push to registry\ndocker push your-registry.com/rock-survival:v2.3.0\nIn the case of using Docker Hub, you can use the following command to push your image to the registry:\ndocker tag rock-survival:v2.3.0 your-dockerhub-username/rock-survival:v2.3.0\ndocker push your-dockerhub-username/rock-survival:v2.3.0\n\n\n\nTroubleshooting image builds\n\nCommon build issues\nPackage installation failures:\n# Add error handling\nRUN Rscript -e \"\n    tryCatch({\n        remotes::install_github('datashield/dsBase', lib = '$ROCK_LIB')\n    }, error = function(e) {\n        cat('Error installing dsBase:', e$message, '\\n')\n        quit(status = 1)\n    })\n\"\nPermission issues:\n# Ensure proper ownership at each step\nRUN Rscript -e \"remotes::install_github('datashield/dsBase', lib = '$ROCK_LIB')\" \\\n    && chown -R rock $ROCK_LIB\nBuild context too large:\n# Check build context size\ndu -sh .\n\n# Use .dockerignore to exclude large files\necho \"*.log\" &gt;&gt; .dockerignore\necho \"data/\" &gt;&gt; .dockerignore\n\n\nDebugging build failures\n# Build with no cache to see all steps\ndocker build --no-cache -f Dockerfile.survival .\n\n# Interactive debugging\ndocker run -it --rm datashield/rock-base:6.3-R4.3 /bin/bash\n\n# Check intermediate layers\ndocker build -f Dockerfile.survival -t debug-build .\ndocker run -it debug-build /bin/bash\n\n\n\nBest practices\n\nVersion everything: Pin package versions and base image tags\nUse multi-stage builds: Keep final images small\nLayer efficiently: Order Dockerfile commands by change frequency\nTest thoroughly: Automate testing of built images\nDocument well: Include package versions and build instructions\n\n\n\nReferences\n\nDataSHIELD Docker images: docker-rock",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>5. Creating custom Rock server images</span>"
    ]
  }
]